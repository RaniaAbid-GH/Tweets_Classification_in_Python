{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets analysis with K-Means clustering using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Involves grouping tweets into 3 categories: ***Computer Science, Telecommunication*** and ***Electronic*** then performing K-Means clustering on the grouped tweets to observe how they behave.\n",
    "* There are several ways of approaching the problem of grouping the tweets, the adapted approach uses the technique of creating a set of words that can be confidently classified as belonging to a particular category for each of the 3 classes. \n",
    "* So the tweets are each compared with the 3 sets and assigned a similarity score. There are 2 main techniques I considered for computing similarity score:\n",
    "    1. Cosine Similarity : Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. This would involve creating word vectors for the set of words and all the tweets then performing the cosine similarity. TFIDF (bag of words model) Vectorizer would be ideal for this.\n",
    "    2. Jaccard Similarity : Jaccard similarity is defined as size of intersection divided by size of union of two sets. Jaccard similarity takes only unique set of words for each sentence/document while cosine similarity takes total length of the vectors. Jaccard similarity is good for cases where duplication does not matter, cosine similarity is good for cases where duplication matters while analyzing text similarity. => In our case, context matters more than duplication thus making Jaccard similarity the most appropriate method to use.\n",
    "* After classifiying the tweets K-Means clustering comes in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tweepy\n",
    "# !pip install spacy --user\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import en_core_web_sm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "%matplotlib inline\n",
    "import logging\n",
    "logging.captureWarnings(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to Twitter’s API using Tweepy / Verifying credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it beneficial to access Twitter API?\n",
    "\n",
    "* Tweets are by nature short-form and contain diverse and relevant topics, so by using Twitter API you get the type, volume and ‘newness’ of data.\n",
    "* Access to Twitter’s API can be acheived by Tweepy which is an open source package that allows you to bypass a lot of those low level details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('ilfVmhYf4DQPjs47LvGuGCk1l', '7RYn8vhws2RkHe3ZneYRlWmTuLmTdZ18kwaRQWSBnshI9C04UG')\n",
    "auth.set_access_token('1331275341013331973-Xq6nyayTyQZTOzpXEMBvL1xi1n6aOS', 'mxT65H33YrWQkVjAlTOtQ8fh2FpQufEbwihpTRvRPmFzV')\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the user object for twitter\n",
    "user = api.get_user('RaniaAbid10')\n",
    "print(user.screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Computer_science_dataset_'+(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv'\n",
    "with open (filename, 'w', newline='',encoding=\"utf-8\") as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['date', 'tweetId','name','tweet','created_at','location'])\n",
    "    # using tweepy Cursor\n",
    "    for tweet in tweepy.Cursor(api.search, q='computer science', lang = 'en', count=50).items():\n",
    "    # writing a csv file\n",
    "        tweets_encoded = tweet.text.encode('utf-8')\n",
    "        tweets_decoded = tweets_encoded.decode('utf-8')\n",
    "        csvWriter.writerow([datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M\"), tweet.id, tweet.user.screen_name, tweets_decoded, tweet.created_at, tweet._json[\"user\"][\"location\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Telecommunication_dataset_'+(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv'\n",
    "with open (filename, 'w', newline='',encoding=\"utf-8\") as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['date', 'tweetId','name','tweet','created_at','location'])\n",
    "    # using tweepy Cursor\n",
    "    for tweet in tweepy.Cursor(api.search, q='telecommunication', lang = 'en', count=50).items():\n",
    "    # writing a csv file\n",
    "        tweets_encoded = tweet.text.encode('utf-8')\n",
    "        tweets_decoded = tweets_encoded.decode('utf-8')\n",
    "        csvWriter.writerow([datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M\"), tweet.id, tweet.user.screen_name, tweets_decoded, tweet.created_at, tweet._json[\"user\"][\"location\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Electronic_dataset_'+(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv'\n",
    "with open (filename, 'w', newline='',encoding=\"utf-8\") as csvFile:\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['date', 'tweetId','name','tweet','created_at','location'])\n",
    "    # using tweepy Cursor\n",
    "    for tweet in tweepy.Cursor(api.search, q='electronic', lang = 'en', count=50).items():\n",
    "    # writing a csv file\n",
    "        tweets_encoded = tweet.text.encode('utf-8')\n",
    "        tweets_decoded = tweets_encoded.decode('utf-8')\n",
    "        csvWriter.writerow([datetime.datetime.now().strftime(\"%Y-%m-%d  %H:%M\"), tweet.id, tweet.user.screen_name, tweets_decoded, tweet.created_at, tweet._json[\"user\"][\"location\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging collected datasets into one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "computer_science_df= pd.read_csv('Datasets_Example2/Computer_science_dataset_2020-12-15.csv')\n",
    "telecommunication_df= pd.read_csv('Datasets_Example2/Telecommunication_dataset_2020-12-15.csv')\n",
    "electronic_df= pd.read_csv('Datasets_Example2/Electronic_dataset_2020-12-15.csv')\n",
    "# display the size of each dataset (rows and columns)\n",
    "print('Computer science dataset size:', computer_science_df.shape)\n",
    "print('Telecommunication dataset size:', telecommunication_df.shape)\n",
    "print('Electronic dataset size:', electronic_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate \n",
    "os.chdir('Datasets_Example2')\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "# combine all files in the list\n",
    "combined_csv = pd.concat(([pd.read_csv(f) for f in all_filenames]))\n",
    "# export to csv\n",
    "combined_csv.to_csv( \"Collected_dataset.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data / Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text data can be sourced from difference places, it can be web scraped and it may also come from online documentation. Text preprocessing is essential in order to further manipulate your text data. In natural language processing, one thing to keep in mind is that whatever you do to the raw data may have an impact on how your model will be trained.\n",
    "* NLTK is a suite of libraries which will help tokenize (break down) text into desired pieces of information (words and sentences). It also allows stemming and lemmatization (normalization techniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load collected data\n",
    "def load_data():\n",
    "    data = pd.read_csv('Collected_dataset.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a concise summary about the data\n",
    "tweets_df = load_data()\n",
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return some first rows to take a look at the data\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data description\n",
    "tweets_df['created_at'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* day with highest number ***14th December 2020***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction of needed columns from the dataframe\n",
    "df = tweets_df[['tweetId', 'name', 'tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "df['tweet_punct'] = df['tweet'].apply(lambda x: remove_punct(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The meaning of tokenization is to chop up some existing text into smaller chunks. For example, a paragraph can be tokenized into sentences and further into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split(' ', text)\n",
    "    return text\n",
    "\n",
    "df['tweet_tokenized'] = df['tweet_punct'].apply(lambda x: tokenization(x.lower()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stopwords with the NLTK module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "df['tweet_nonstop'] = df['tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is used to normalize parts of text data. What does this mean exactly?\n",
    "> When you are using a verb which is conjugated in multiple tenses throughout a document you would like to process, stemming will shorten all of these conjugated verbs to the shortest length of characters possible, it will preserve the root of the verb in this case. Stemming is done for all types of words and adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df['tweet_stemmed'] = df['tweet_nonstop'].apply(lambda x: stemming(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lemmatization is another normalization technique which is used in Natural Language Processing. The difference between stemming and lemmatization is that lemmatization will enable for words which do not have the same root to be grouped together in order for them to be processed as one item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df['tweet_lemmatized'] = df['tweet_nonstop'].apply(lambda x: lemmatizer(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving extracted cleaned data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df = pd.DataFrame(df[['tweetId', 'name', 'tweet_lemmatized']])\n",
    "extracted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df.to_csv('Cleaned_data.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset verification\n",
    "new_tweet_df= pd.read_csv('Cleaned_data.csv')\n",
    "print('Dataset size:',new_tweet_df.shape)\n",
    "print('Columns are:',new_tweet_df.columns)\n",
    "new_tweet_df.info()\n",
    "new_tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the sets of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_science_related_words = '''computer computation computing software algorithm hardware processor tested digital logic code information computer science CPU security system mainframe technology architecture interface survey surveyed experience improve improvement capability service link engineering program platform computationalsecurity virtual machine VM revolution approved softmodem telnet importance test minicomputer pseudocode new technologies supercomputer microprocessor screensaver pixel cyberintrusion information connection connectivity processing system microcomputer computing device computing machine data processor precomputer development cyber innovation IOT social networks communities data flow techie cybernetic computerology cybersystem computer-aid design computerist cybertechnology neurocomputer computerbased cyberinteraction network multinetworked multinetwork antivirus computerologist wardialer nanocomputer noncomputer business management downloader login logout uploader cybernetwork computerdom computerism compy virtualize bios ibook hack computable programmable telecommuter core computerize institutions values cyberterrorism incrementor  utility program PDA telecomputer computerlike computercide intranet cybersuicide configuration section central process unit download upgrade streaming manager database management rootkit command line interpreter cybergeneration digital communication data process data converter word process cyberfuture organize information web server install uninstall graphician machine code terminal access internet extranet turing machine teleinstruction graphic card open source stream video background process meatware scancode word processor killer poke memory chip sneakernet brain process information expansion slot surf internet at internet tv keyboard escape key computer graphic artificial intelligence computer visual display unit alpha test video card information superhighway data path etransaction technical information cybernetics programming language computational complexity theory informatics artificial intelligence  cace interoperability access computational IBM compiler debug assembler parser computer simulation computers programs technologies interactive organise encode computer engineering numerical analysis architecture prompt port disc foreground descriptor object firewall panel drive pointer ip counter document ontology risc video email track server package bbs bpi backup buffer argument filename parameter host address accumulator reference desktop background menu ascii dump microcode directory router register printer ram rom simulation node disk machine window spyware malware programmer eprom subdirectory CPU positioner cisc dongle e-mail submenu research laptop nonprogramming cyber machinima scientific systems lab tool predictor imac developed digital mac arithmometer business cyberpunk creative based tools AI technical science partnership department knowledge software engineer developer quality for everyone analysis natural  language processing cryptography mining progress progressing vision google microsoft apple facebook messenger amazon investment personal information editor resource applications calculator web system tech data labs work educational focuses internet communication multimedia sciences design focused expert bloatware using new modern secure online offline multitask technological multiprogramming concepts counterprogramming harvard instance misclick researchers graduate expertise innovative teleprogramming informatic innovation scientist device sophisticated experimental optimizer teleprogrammed subprogram cybernate multiload computer scientist web descktop computational problem mouseclick preprogram groupware processmaker computer graphics bitness phone mobile cyber programming language theory cyberdating fileserver computer programming deep learning modeling IoT java R C python SQL SPARQ html CSS UML merise framework iphone Samsung'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecommunication_related_words = '''telecommunication telephone phone mobile communication telecom wireless telephony telegraph telegraphy connect connection link smartpone line communications infrastructure radio internet optical fiber transmission call teleprinter duplex radiotelephone broadcasting cellphon intercommunication telecoms teletype services mobile broadband providers sectors operators telecommunications networks cellular session active activate satellite international telecommunication union teleoperator orange SFR bouygues gelgacom connectivity dial tone ECC transmission handset cellular telephone gsm voip asynchronous transfer mode noise signalling signal telephones interconnection wire emission information electromagnetic social socialized communities local area network entities sampling heliograph medium multiplex simplex iphone quantize telephonic telecommunicate phone smartphone propagation megaphone speaker miscommunication modem dataphone cybercommunication webphone systems electrocommunication webcam cellevision corp selfridges netphone allophone phonetic radio regulations BTS LTD navigation technology incommunicable radiotelegraph technologies communicative communicational electrical untelephoned speakerphone convention ict magnetotelephone telephoner operating phoneless unphoned autodialer telegraphic upgrading sector telephonable overcommunication networking teletex communication channel communicate shielded cable grid manufactures telkom textphone operator electromagnetic radiation phonable terminals integrated subsidiaries monopoly operates multimedia signal access automated trimphone satelite cable phoner telepathize monopolies system siemens contact conglomerate callback ethernet switchboard telegram poldhu telecommunication network liaison contacts communicator transmission receiver fiber-optic communication nyquist frequency communications satellite gain error rate frequency nyquist dial wireless communication etisalat zain interconnect modems audiovisual nilesat hotbird intelsat cabling antenna transmit transmitter uncommunicated kilowatt miscommunicate remote access LTD pocketphone italophone multiplexing state-owned telefelony voice mail unicom telephone system communication device router attenuation keyphone carrier wave germanophone telematic radiolocation fiberoptics radiocommunication digitalization upcall telephone receiver mobile telephone infrared downcall code radiogram telegraph cable channel mobile phone make call telephone operator ecommunication nanowatt dial telephone mhz hz bandwidth frequency wireless telegraphy modulation telegraph signals box telegraph bluetooth utility  tesla Samsung Apple Nokia Huawei Oppo call box call originator address message disk television make phone call network broadcasting corporation inscribe email wireless operator call in dial phone telephone mainframe symetric asymetric cellular phone dial switching request comment switch access point telephone box transmission control protocol Messenger WhatsApp call sign propaganda token ring visual communication telephone tag communication system radio station power amplifier send message messenger whatsapp channel social media fibre optic cable fiber optics fiber optic cable optical fibre telecommunication equipment subscriber line phone service fibre optics exchange idea electrical device data center telephone line call radio receiver exchange information microphone FTTX FTTH Huawei Cisco ADSL WIFI ethernet'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electronic_related_words = '''digital analog computer electrical wireless portable electronic engineer engineering virtual device electromechanical interactive audio information hardware electric communication machine compnents devices systems electronically technology audiovisual computerization machines instruments sensors data multimedia equipment processing tool uses rapid tools scanning standard allows components use embedded manufacturers materials system editing phones enables users decoder development instrument installation application machine recording consumer commercial communication computerised micro phone cellphone communications technologies screen electro cybernetics electrician electrotechnical encrypted electricity kinetic energy power radiant amp energy heat chemical energy force joule vigor potential energy thermal energy heat engine entropy fuel mechanical energy efficiency vim activation energy physical phenomenon heating qi energy level focused new goals renewable energy biomass radiation conservation of energy alternative energy geothermal energy watt nuclear fission gas machinima taser telnet real-time transistor oscilloscope autodialer cyborg portability bios processor board electricity pda cpu electrode transmitter mac microprocessor electrify cyberinteraction netlag elektronik electrical device electrocommunication computerology electronic device third screen computer hardware radio frequency equipment biometric identification cathode information stream  information matrix printer automated authentication explosive detection system global positioning system thermometer security system personal digital robots robot best brain circuit sustainable distribute power AI supply error machine data center alternate current analog remote control electromagnetic pulse graphic card machine data process computer science test virtual reality technical support download manager visual display unit surf electronic electro mechanical frequency identification rfid devices digitized tamper proof auditable contactless biometrics  tags CD DVD hitech touchscreen wireless implementation implement implantable laptop computers texas instruments VHDL ARDUINO Matlab Simulink C python R DOS cables industry ASUS HP Lenovo Apple NVIDIA Intel Sun microsystems AMD Emerson DELL Sony Audit Renault BMW FIAT motor using artificial intellegence AI inttelligent smart work standard IEEE'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenizing, Lemmatizing and removing stopwords from the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# clean the set of words         \n",
    "def furnished(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.lower() not in stop:\n",
    "            word = lemmatizer.lemmatize(i)\n",
    "            final_text.append(word.lower())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "computer_science = furnished(computer_science_related_words)\n",
    "telecommunication = furnished(telecommunication_related_words)\n",
    "electronic = furnished(electronic_related_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. deleting duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = computer_science\n",
    "words = string1.split()\n",
    "computer_science = \" \".join(sorted(set(words), key=words.index))\n",
    "computer_science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string2 = telecommunication\n",
    "words = string2.split()\n",
    "telecommunication = \" \".join(sorted(set(words), key=words.index))\n",
    "telecommunication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string3 = electronic\n",
    "words = string3.split()\n",
    "electronic = \" \".join(sorted(set(words), key=words.index))\n",
    "electronic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing and standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After cleaning the sets, the next step is vectorising and standardizing them. \n",
    "  This is necessary to calculate Cosine Similarity score which is a measure of similarity between two groups (non-zero vectors). Implementing TF-IDF scheme would be perfect for that. \n",
    "* The idea behind TF-IDF approach is that the words that are more common in one sentence and less common in other sentences should be given high weights.\n",
    "\n",
    "> The term TF stands for **\"Term Frequency\"** while the term IDF stands for the **\"Inverse Document Frequency\"**.\n",
    "\n",
    "> **TF** = (Frequency of the word in the sentence) / (Total number of words in the sentence).\n",
    "\n",
    "> **IDF** = (Total number of sentences (document)) / (Number of sentences (document) containing the word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing the sets\n",
    "def get_vectors(*strs):\n",
    "    text = [t for t in strs]\n",
    "    # create TfidfVectorizer object\n",
    "    vectorizer = TfidfVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "\n",
    "computer_science_vector = get_vectors(computer_science)\n",
    "telecommunication_vector = get_vectors(telecommunication)\n",
    "electronic_vector = get_vectors(electronic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing the tweets\n",
    "tv=TfidfVectorizer()\n",
    "tfidf_tweets =tv.fit_transform(new_tweet_df.tweet_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Jaccard similarity index measures the similarity between two sets of data. It can range from 0 to 1. \n",
    "  The higher the number, the more similar the two sets of data.\n",
    "* Jaccard similarity is good for cases where duplication does not matter, cosine similarity is good for cases where duplication matters while analyzing text similarity. In our case, it would be better to use Jaccard similarity as repetition of a word does not reduce their similarity.\n",
    "* The Jaccard similarity index is calculated as :\n",
    "\n",
    "> **Jaccard Similarity** = (number of observations in both sets) / (number in either set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Jaccard Similarity function\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Scores fuction to find Jaccard Similarity between the two defined sets \n",
    "def get_scores(group,tweets):\n",
    "    scores = []\n",
    "    for tweet in tweets:\n",
    "        s = jaccard_similarity(group, tweet)\n",
    "        scores.append(s)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computer science scores \n",
    "cs_scores = get_scores(computer_science, new_tweet_df.tweet_lemmatized.to_list())\n",
    "cs_scores[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# telecommunication scores\n",
    "t_scores = get_scores(telecommunication, new_tweet_df.tweet_lemmatized.to_list())\n",
    "t_scores[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electronic scores\n",
    "e_scores = get_scores(electronic, new_tweet_df.tweet_lemmatized.to_list())\n",
    "e_scores[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a new dataframe containing names, and the jaccard scores for each group.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = {'tweet':new_tweet_df.tweet_lemmatized.to_list(), 'computer_science_score': cs_scores,\n",
    "         'telecommunication_score':t_scores, 'electronic_score':e_scores}\n",
    "scores_df = pd.DataFrame(data)\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There's a thin line between computer science and electronic scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual assigning of classes to the tweets\n",
    "def get_clusters(l1, l2, l3):\n",
    "    comp_sc = []\n",
    "    tel = []\n",
    "    elec = []\n",
    "    for i, j, k in zip(l1, l2, l3):\n",
    "        m = max(i, j, k)\n",
    "        if m == i:\n",
    "            comp_sc.append(1)\n",
    "        else:\n",
    "            comp_sc.append(0)\n",
    "        if m == j:\n",
    "            tel.append(1)\n",
    "        else:\n",
    "            tel.append(0)        \n",
    "        if m == k:\n",
    "            elec.append(1)\n",
    "        else:\n",
    "            elec.append(0)               \n",
    "    return comp_sc, tel, elec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = scores_df.computer_science_score.to_list()\n",
    "l2 = scores_df.telecommunication_score.to_list()\n",
    "l3 = scores_df.electronic_score.to_list()\n",
    "comp_sc, tel, elec = get_clusters(l1, l2, l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': new_tweet_df.name.to_list(), 'computer_science':comp_sc, 'telecommunication': tel, 'electronic':elec}\n",
    "cluster_df = pd.DataFrame(data)\n",
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustered dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping the tweets by username\n",
    "pivot_clusters = cluster_df.groupby(['name']).sum()\n",
    "pivot_clusters['computer_science'] = pivot_clusters['computer_science'].astype(int)\n",
    "pivot_clusters['telecommunication'] = pivot_clusters['telecommunication'].astype(int)\n",
    "pivot_clusters['electronic'] = pivot_clusters['electronic'].astype(int)\n",
    "# add a new total column\n",
    "pivot_clusters['total'] = pivot_clusters['computer_science'] + pivot_clusters['telecommunication'] +  pivot_clusters['electronic']\n",
    "# add a new total row\n",
    "pivot_clusters.loc[\"Total\"] = pivot_clusters.sum() \n",
    "print(pivot_clusters.shape)\n",
    "pivot_clusters.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie chart showing the total number of tweets in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(10, 7)) \n",
    "a = pivot_clusters.drop(['total'], axis = 1)\n",
    "colors_list=['deepskyblue', 'sandybrown', 'yellowgreen']\n",
    "plt.pie(a.loc['Total'], labels = a.columns, colors = colors_list)\n",
    "plt.title('A pie chart showing the volumes of tweets under different categories')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plot showing users with most tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pivot_clusters.sort_values(by = 'total', ascending  = False)\n",
    "e = d.head(12)\n",
    "e.drop(e.head(2).index, inplace=True)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x = e.index, y = e.total)\n",
    "plt.title('A bar plot showing top tweets based on volume of tweets')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Names')\n",
    "plt.ylabel('Total tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the totals row\n",
    "print(pivot_clusters.shape)\n",
    "pivot_clusters.drop(pivot_clusters.tail(1).index,inplace=True)\n",
    "print(pivot_clusters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Distribution per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pivot_clusters['computer_science']);\n",
    "plt.title('Computer science tweets distribution plot')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pivot_clusters['telecommunication']);\n",
    "plt.title('Telecommunication tweets distribution plot')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pivot_clusters['electronic']);\n",
    "plt.title('Electronic tweets distribution plot')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Teleinformatics (telecommunication & computer science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x ='computer_science', y ='telecommunication', data = pivot_clusters, kind=\"reg\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. IoT (electronic & automation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x ='computer_science', y ='electronic', data = pivot_clusters, kind ='reg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The K-Means algorithm needs no introduction. It is simple and perhaps the most commonly used algorithm for clustering.\n",
    "* The Elbow Method is probably the most well-known method for determining the optimal number of clusters. \n",
    "* Calculate the Within-Cluster-Sum of Squared Errors (WCSS) for different values of k, and choose the k for which WCSS becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pivot_clusters[['computer_science', 'telecommunication','electronic']].values\n",
    "# elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    # building and fitting the model \n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The plot looks like an arm with a clear elbow at k = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=150, c='violet', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=150, c='orange', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=150, c='chocolate', label= 'Cluster 3')\n",
    "plt.scatter(X[Y_kmeans==3, 0], X[Y_kmeans==3, 1], s=150, c='deepskyblue', label= 'Cluster 4')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=40, c='black', label='Centroids' )\n",
    "plt.title('Data Clustering')\n",
    "plt.xlabel('Computer science tweets')\n",
    "plt.ylabel('Telecommunication tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Teleinformatics (telecommunication & computer science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pivot_clusters[['computer_science', 'telecommunication']].values\n",
    "# elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    # building and fitting the model \n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "# visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=150, c='yellowgreen', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=150, c='gold', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=150, c='chocolate', label= 'Cluster 3')\n",
    "plt.scatter(X[Y_kmeans==3, 0], X[Y_kmeans==3, 1], s=150, c='violet', label= 'Cluster 4')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=50, c='black', label='Centroids' )\n",
    "plt.title('Clusters of tweets in computer science and telecommunication groups')\n",
    "plt.xlabel('Computer science tweets')\n",
    "plt.ylabel('Telecommunication tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. IoT (electronic & automation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pivot_clusters[['computer_science', 'electronic']].values\n",
    "# elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting kmeans to dataset\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "Y_kmeans = kmeans.fit_predict(X)\n",
    "# visualising the clusters\n",
    "plt.scatter(X[Y_kmeans==0, 0], X[Y_kmeans==0, 1], s=150, c='deepskyblue', label= 'Cluster 1')\n",
    "plt.scatter(X[Y_kmeans==1, 0], X[Y_kmeans==1, 1], s=150, c='sandybrown', label= 'Cluster 2')\n",
    "plt.scatter(X[Y_kmeans==2, 0], X[Y_kmeans==2, 1], s=150, c='greenyellow', label= 'Cluster 3')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=50, c='black', label='Centroids' )\n",
    "plt.title('Clusters of tweets in health and social groups')\n",
    "plt.xlabel('Electronic tweets')\n",
    "plt.ylabel('Automation tweets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  KMeans clustering with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To make the clustering more apparent, we can use the K-means algorithm with PCA.\n",
    "* PCA (Principal Components Analysis) reduces the number of features in our data set. By reducing the number of features, we’re improving the performance of our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed import for dimension reduction\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we must fit our standardized data using PCA\n",
    "seg = pivot_clusters.copy()\n",
    "pca = PCA()\n",
    "pca.fit(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, we need to decide how many features we’d like to keep based on the cumulative variance plot\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(range(1,5), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle = '--')\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Cummulative explained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The graph shows the amount of variance captured (on the y-axis) depending on the number of components we include (the x-axis). A rule of thumb is to preserve around 80 % of the variance. So, in this instance, we decide to keep 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "pca.fit(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pca.transform(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow Method\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "# creating 15 K-Means models while varying the number of clusters (k)\n",
    "for k in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
    "     # fit model to samples\n",
    "    kmeans.fit(scores)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,15), wcss)\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('KMeans with PCA clustering')\n",
    "plt.ylabel('WCSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "kmeans_pca = KMeans(n_clusters = n, init = 'k-means++', random_state = 0)\n",
    "kmeans_pca.fit(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.concat([seg.reset_index(drop = True), pd.DataFrame(scores)], axis = 1)\n",
    "c.columns.values[-2:] = ['component1', 'component2']\n",
    "c['segment_kmeans_pca'] = kmeans_pca.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,5))\n",
    "sns.scatterplot(x = c['component1'], y = c['component2'], hue = c['segment_kmeans_pca'], palette = ['y','c','b','g'])\n",
    "plt.title('Clusters by PCA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
